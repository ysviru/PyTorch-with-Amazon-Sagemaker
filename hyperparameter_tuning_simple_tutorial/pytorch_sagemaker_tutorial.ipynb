{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use hyperparameter tuning with SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create synthetic data for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 make_classification() from sklearn provides a handy way to generate synthetic dataset for classification task. In this case, we define a dataset with 15 input features and 3 output classes. Using the train_test_split() frrom sklearn.model_selection, we create the train, test and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=10000, n_features=15, n_informative=10, \n",
    "                             n_redundant=5, n_classes=3, n_clusters_per_class=2, \n",
    "                             class_sep=1.5, flip_y=0.01, weights=[0.5, 0.5, 0.5])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Convert the numpy arrays to pandas dataframes and store as csvs to data/ folder. We can then upload this folder to our S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "\n",
    "X_val_df = pd.DataFrame(X_val)\n",
    "y_val_df = pd.DataFrame(y_val)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test)\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "\n",
    "X_train_df.to_csv(\"data/training/X_train.csv\", index=False)\n",
    "y_train_df.to_csv(\"data/training/y_train.csv\", index=False)\n",
    "\n",
    "X_val_df.to_csv(\"data/training/X_validation.csv\", index=False)\n",
    "y_val_df.to_csv(\"data/training/y_validation.csv\", index=False)\n",
    "\n",
    "X_test_df.to_csv(\"data/test/X_test.csv\", index=False)\n",
    "y_test_df.to_csv(\"data/test/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sagemaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Define session, bucket and role. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/pytorch-synthetic\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Upload data to S3 bucket. \n",
    "\n",
    "Upload the data/ folder to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-2-046610044696/sagemaker/pytorch-synthetic\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path=\"data/training\", bucket=bucket, key_prefix=prefix)\n",
    "print(\"input spec (in this case, just an S3 path): {}\".format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Training in Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"pytorch_synthetic_data_entry.py\",\n",
    "                    role=role,\n",
    "                    framework_version=\"1.1.0\",\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type=\"ml.c4.xlarge\",\n",
    "                    hyperparameters={\n",
    "                        \"num-epochs\": 10, \n",
    "                        \"learning-rate\": 0.005,\n",
    "                        \"batch-size\": 32, \n",
    "                        \"test-batch-size\": 32\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-20 13:40:33 Starting - Starting the training job...\n",
      "2019-09-20 13:40:37 Starting - Launching requested ML instances...\n",
      "2019-09-20 13:41:31 Starting - Preparing the instances for training......\n",
      "2019-09-20 13:42:17 Downloading - Downloading input data...\n",
      "2019-09-20 13:42:56 Training - Training image download completed. Training in progress..\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-09-20 13:42:58,122 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[31m2019-09-20 13:42:58,125 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-09-20 13:42:58,136 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:01,151 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:01,425 sagemaker-containers INFO     Module pytorch_synthetic_data_entry does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:01,425 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:01,425 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:01,425 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: pytorch-synthetic-data-entry\n",
      "  Running setup.py bdist_wheel for pytorch-synthetic-data-entry: started\n",
      "  Running setup.py bdist_wheel for pytorch-synthetic-data-entry: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oir0mr8m/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built pytorch-synthetic-data-entry\u001b[0m\n",
      "\u001b[31mInstalling collected packages: pytorch-synthetic-data-entry\u001b[0m\n",
      "\u001b[31mSuccessfully installed pytorch-synthetic-data-entry-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.2.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:03,424 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:03,436 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 32,\n",
      "        \"test-batch-size\": 32,\n",
      "        \"learning-rate\": 0.005,\n",
      "        \"num-epochs\": 10\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-pytorch-2019-09-20-13-40-33-045\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-046610044696/sagemaker-pytorch-2019-09-20-13-40-33-045/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"pytorch_synthetic_data_entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"pytorch_synthetic_data_entry.py\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[31mSM_HPS={\"batch-size\":32,\"learning-rate\":0.005,\"num-epochs\":10,\"test-batch-size\":32}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=pytorch_synthetic_data_entry.py\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=pytorch_synthetic_data_entry\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-2-046610044696/sagemaker-pytorch-2019-09-20-13-40-33-045/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":32,\"learning-rate\":0.005,\"num-epochs\":10,\"test-batch-size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-pytorch-2019-09-20-13-40-33-045\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-046610044696/sagemaker-pytorch-2019-09-20-13-40-33-045/source/sourcedir.tar.gz\",\"module_name\":\"pytorch_synthetic_data_entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"pytorch_synthetic_data_entry.py\"}\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--batch-size\",\"32\",\"--learning-rate\",\"0.005\",\"--num-epochs\",\"10\",\"--test-batch-size\",\"32\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31mSM_HP_BATCH-SIZE=32\u001b[0m\n",
      "\u001b[31mSM_HP_TEST-BATCH-SIZE=32\u001b[0m\n",
      "\u001b[31mSM_HP_LEARNING-RATE=0.005\u001b[0m\n",
      "\u001b[31mSM_HP_NUM-EPOCHS=10\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pytorch_synthetic_data_entry --batch-size 32 --learning-rate 0.005 --num-epochs 10 --test-batch-size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 1, Loss: 1.097149\u001b[0m\n",
      "\u001b[31mValidation: accuracy=36.875\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.033924425318837166\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0339, Accuracy: 590/1600 (37%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 2, Loss: 1.077158\u001b[0m\n",
      "\u001b[31mValidation: accuracy=45.75\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.03333970114588738\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0333, Accuracy: 732/1600 (46%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 3, Loss: 1.058462\u001b[0m\n",
      "\u001b[31mValidation: accuracy=52.6875\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.032760493978857996\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0328, Accuracy: 843/1600 (53%)\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 4, Loss: 1.038221\u001b[0m\n",
      "\u001b[31mValidation: accuracy=56.875\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.03210959635674954\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0321, Accuracy: 910/1600 (57%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 5, Loss: 1.017326\u001b[0m\n",
      "\u001b[31mValidation: accuracy=60.375\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.03150231927633285\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0315, Accuracy: 966/1600 (60%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 6, Loss: 1.001238\u001b[0m\n",
      "\u001b[31mValidation: accuracy=62.625\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.03108193326741457\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0311, Accuracy: 1002/1600 (63%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 7, Loss: 0.990082\u001b[0m\n",
      "\u001b[31mValidation: accuracy=64.0625\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.03080696366727352\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0308, Accuracy: 1025/1600 (64%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 8, Loss: 0.981686\u001b[0m\n",
      "\u001b[31mValidation: accuracy=65.5\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.030594832189381124\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0306, Accuracy: 1048/1600 (66%)\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 9, Loss: 0.975108\u001b[0m\n",
      "\u001b[31mValidation: accuracy=66.6875\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.030419334024190902\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0304, Accuracy: 1067/1600 (67%)\n",
      "\u001b[0m\n",
      "\u001b[31mTrain epoch: 10, Loss: 0.969762\u001b[0m\n",
      "\u001b[31mValidation: accuracy=67.25\u001b[0m\n",
      "\u001b[31mValidation: loss = 0.03027341913431883\u001b[0m\n",
      "\u001b[31mTest set: Average loss: 0.0303, Accuracy: 1076/1600 (67%)\n",
      "\u001b[0m\n",
      "\u001b[31m2019-09-20 13:43:07,384 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-09-20 13:43:17 Uploading - Uploading generated training model\n",
      "2019-09-20 13:43:17 Completed - Training job completed\n",
      "Training seconds: 60\n",
      "Billable seconds: 60\n"
     ]
    }
   ],
   "source": [
    "# Synchronous \n",
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Hyperparameter tuning in Sagemaker\n",
    "\n",
    "The HyperparameterTuner class performs the hyperparameter tuning for us. \n",
    "1. We first define all the different inputs to the HyperparameterTuner class.\n",
    "2. Next, we supply the estimator object along with inputs defined in 1. and create the tuner instance.\n",
    "3. We call the tuner's fit function similar to how we called estimator's fit function. \n",
    "\n",
    "Note: The key for this to work is --- to log the test_loss variable inside of test() function in the pytorch_synthetic_data_entry.py script. The logger.info() function inside the test() function uses the string \"Test set: Average loss:\" to log the loss value. This string must match the one provided in the \"Regex\" component for the metric_definitions variable. \n",
    "\n",
    "If we do not log the test_loss variable inside the test() function, Sagemaker cannot make a decision on which hyperparameter configuration gives the best result and thus the tuning cannot successfully complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import CategoricalParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\"num-epochs\": CategoricalParameter([5, 10])}\n",
    "objective_metric_name = 'average test loss'\n",
    "objective_type = 'Minimize'\n",
    "metric_definitions = [{'Name': 'average test loss',\n",
    "                       'Regex': 'Test set: Average loss: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=2,\n",
    "                            max_parallel_jobs=2,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous \n",
    "tuner.fit({\"training\": inputs})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
